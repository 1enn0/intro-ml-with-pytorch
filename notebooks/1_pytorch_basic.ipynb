{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/\"><img src=\"https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png\" width=\"600px\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PyTorch** (1/2): the world's simplest neural net from scratch\n",
    "\n",
    "In this notebook, we will build a simple neural network to learn a linear equation with one variable a.k.a. a _line_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch?\n",
    "* open-source machine learning library\n",
    "* developed by Facebook AI Research + community\n",
    "* used in most state-of-the-art research\n",
    "* \"Python first\": deeply integrated into Python, it should \"feel familiar\" to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Components\n",
    "* ```torch```: tensor library (like NumPy but with GPU support)\n",
    "* ```torch.autograd```: automatic differentiation library\n",
    "* ```torch.jit```: compile PyTorch code to TorchScript for deployment (e.g. in standalone C++ program)\n",
    "* ```torch.nn```: neural network library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "* [ðŸ”— Official PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "Tutorials:\n",
    "* [ðŸ”— 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* [ðŸ”— PyTorch Tutorials with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution\n",
    "\n",
    "Most of the material in the theoretical part of this document is taken from the fast.ai [Practical Deep Learning for Coders](https://course.fast.ai/) notebooks.\n",
    "\n",
    "In you are interested, check out the colab notebooks from the fast.ai course, especially\n",
    "* [01_intro](https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb) and \n",
    "* [04_mnist_basics](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb). \n",
    "\n",
    "They cover a lot of fundamentals of neural networks and machine learning in general in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network to Learn a Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the ML introduction, we need some data to train our model.\n",
    "\n",
    "In our case, we can just generate data points for an arbitrary line function. In order to do this we first need to define our _target_, i.e. the line equation that we want our model to learn. \n",
    "\n",
    "As you might recall, a _line_ is defined by the equation $$ y = ax + b $$\n",
    "\n",
    "It has one variable ($x$) and two parameters:\n",
    "* $a$ (_slope_) and \n",
    "* $b$ (_intercept_). \n",
    "\n",
    "The _parameters_ are what we want our model to learn.\n",
    "\n",
    "Let's create an arbitrary line function by generating some random values for $a$ and $b$.\n",
    "\n",
    "When developing algorithms that contain any kind of randomness, for reproducibility it is always a good idea to set a _manual seed_ before you start. This will initialize the random number generators to the same state every time you run your experiment, i.e. you will always get the same results.\n",
    "\n",
    "In NumPy, you can do this by passing an arbitrary number to the function `numpy.random.seed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.random.uniform(low=-2.0, high=2.0, size=2)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use these to define our target function $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our _independent variable_ $x$ by generating 1000 evenly spaced numbers over the interval $[-\\pi, \\pi]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000\n",
    "\n",
    "x = np.linspace(-np.pi, np.pi, n_samples, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of samples we generate will influence our results: the more data points we allow the model to learn from, the better our final performance will be.\n",
    "\n",
    "Let's create our _dependent variable_ $y$ by passing $x$ to our function $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, any data that you obtain will include some kind of noise (e.g. slight deviations in temperature readings due to a given precision of the sensor itself). Luckily, neural networks can handle noisy data quite well. Actually, introducing noise during training (especially for small datasets) can improve the robustness of the network and result in a better generalization (see e.g. [here](https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/)).\n",
    "\n",
    "So, let's go ahead and add some noise to our _true_ $y$ data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can draw some samples from a normal distribution and add it to our true $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.random.normal(size=y_true.shape).astype(np.float32)\n",
    "y_noisy = y_true + e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what our line looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y_noisy, s=1, color='#90a4ae', alpha=0.75, label='y_noisy')\n",
    "ax.plot(x, y_true, color='#455a64', label='y_true')\n",
    "ax.grid(True, alpha=0.4)\n",
    "ax.legend()\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a neural network to learn this function, we need a model that also has (at least) two parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we need to decide what _kind_ of layers our neural network is made of. Since we want to model a linear equation, `torch.nn.Linear` might be a good fit. Let's have a look at [what this layer does](https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we need! Conceptually, it looks like this:\n",
    "\n",
    "<img src=\"../assets/images/linear_unit.svg\" alt=\"Linear unit\" width=\"400px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $n$ inputs $x_1, x_2, \\dots, x_n$, each of which is multiplied by its corresponding weight $w_1, w_2, \\dots, w_n$. The output $y$ is the weighted sum of all inputs plus a single bias term $b$.\n",
    "\n",
    "For our specific case, we can simplify this to:\n",
    "\n",
    "<img src=\"../assets/images/linear_unit_single.svg\" alt=\"Linear unit with a single input\" width=\"400px\"/>\n",
    "\n",
    "We just have a single input ($x$), the weight corresponds to the slope $a$ and the bias $b$ corresponds to the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct an instance of `torch.nn.Linear`, we need specify the sizes of our input and output samples. In our case, these are just scalar values (i.e. shape `1`).\n",
    "\n",
    "Again, we set a manual seed here because the layer object takes care of randomly initializing its parameters. The PyTorch way of doing this is by calling `torch.manual_seed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed); # for reproducibility\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add a `Flatten` layer at the end. This will flatten the output to a 1D tensor to match the shape of our labels (i.e. the `y_noisy` vector).\n",
    "\n",
    "We can have have a look at our model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(name, tensor.item()) for (name, tensor) in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have two scalar parameters: a _weight_ and a _bias_.\n",
    "\n",
    "Currently, our training data is stored in NumPy arrays. To be able pass the data to our model, we have to convert them to PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.from_numpy(x).unsqueeze(-1)\n",
    "yy = torch.from_numpy(y_noisy)\n",
    "\n",
    "xx.shape, yy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, our model parameters are initialized to random values. \n",
    "\n",
    "Let's see how well our untrained neural network is predicting our line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, figsize=(6, 6), gridspec_kw={'height_ratios':[0.7, 0.3]})\n",
    "axes[0].scatter(x, y_noisy, s=0.5, color='#90a4ae', alpha=0.25, label='y_noisy')\n",
    "axes[0].plot(x, y_true, linewidth=1, label='y_true', color='#455a64')\n",
    "axes[0].set_title('Step 0')\n",
    "line_pred = axes[0].plot(x, model(xx).detach().numpy(), label='y_pred', color='#ffa000')[0]\n",
    "line_loss = axes[1].semilogy(np.arange(n_training_steps)[0], 1000, label='training loss')[0]\n",
    "axes[1].set_xlim([0, n_training_steps])\n",
    "axes[1].set_xlabel('# epoch')\n",
    "axes[1].set_ylabel('mse loss')\n",
    "for ax in axes:\n",
    "    ax.grid(True, which='both', alpha=0.25)\n",
    "    ax.legend()\n",
    "fig.canvas.header_visible = False\n",
    "fig.tight_layout()\n",
    "\n",
    "def update_every(iteration, every = 50):\n",
    "    return iteration % every == every - 1\n",
    "\n",
    "def set_plot_data(y_pred, loss):\n",
    "    line_pred.set_ydata(y_pred)\n",
    "    line_loss.set_data(np.arange(loss.shape[0]), loss)\n",
    "    axes[0].set_title(f'Epoch {t}: loss {loss[-1]:.2f}')\n",
    "    axes[1].set_xlim([0, n_training_steps])\n",
    "    axes[1].relim()\n",
    "    axes[1].autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of the <a href=\"#detailed_loop\">Detailed Training Loop Diagram</a> and compare what we have achieved so far:\n",
    "\n",
    "* [x] Collect input data (our tensor `xx`) and\n",
    "* [x] Corresponding label data (our tensor `yy`)\n",
    "* [x] Define a model architecture\n",
    "* [x] Initialize model parameters to random values\n",
    "* [x] Calculate predictions (our tensor `y_pred`) based on current model parameters and input data\n",
    "* [ ] Define a loss function to determine the performance\n",
    "* [ ] Update model parameters based on loss using automated process (Stochastic Gradient Descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update our model parameters, we need to define a suitable loss function. \n",
    "\n",
    "We want `y_pred` to exactly match `y_true`, so we need a mathematical way of figuring out how _similar_ our prediction is to our target line.\n",
    "\n",
    "We can use the _mean squared error (MSE)_ to achieve this. It is defined like this:\n",
    "\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^2$\n",
    "\n",
    "For every data sample $i$, we compute the squared difference of the target value $Y_i$ and the predicted value $\\hat{Y_i}$. The squared differences are then added up accross all sample and divided by the number of samples to yield a single scalar value, the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Furthermore, we need the automated process that will find values for our model parameters $w$ and $b$, such that the predictions closely resemble our target line. This is what the SGD algorithm does for us.\n",
    "\n",
    "Specifically, these are the steps that are required to make our model learn from its experience:\n",
    "\n",
    "<br>\n",
    "<img src=\"../assets/images/sgd.svg\" alt=\"Stochastic Gradient Descent process\" width=\"700px\"/>\n",
    "<a href=\"https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb\">(Image Source)</a>\n",
    "<br>\n",
    "\n",
    "1. *Initialize* the weights.\n",
    "1. For each input sample, use these weights to *predict* the output value.\n",
    "1. Based on these predictions, calculate how good the model is (its *loss*).\n",
    "1. Calculate the *gradient*, which measures for each weight, how changing that weight would change the loss\n",
    "1. *Step* (that is, change) all the weights based on that calculation.\n",
    "1. Go back to the step 2, and *repeat* the process.\n",
    "1. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don't want to wait any longer).\n",
    "\n",
    "Essentially, we want SGD to find the (_a_) minimum of our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlfromscratch.com/content/images/2019/12/gradient-descent-optimized--1-.gif\" alt=\"SGD\" width=\"600px\"/>\n",
    "\n",
    "Example from [here](https://mlfromscratch.com/optimizers-explained/#/) (animation originally from [3blue1brown](https://www.youtube.com/watch?v=IHZwWFHWa-w))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "\n",
    "n_training_steps = 100\n",
    "losses = np.ones(n_training_steps) * np.infty\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model[0].reset_parameters()\n",
    "\n",
    "t = 0\n",
    "set_plot_data(model(xx).detach().numpy(), losses[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t += 1\n",
    "\n",
    "# run forward pass\n",
    "y_pred = model(xx)\n",
    "\n",
    "# calculate loss\n",
    "loss = loss_fn(y_pred, yy)\n",
    "\n",
    "# update plots\n",
    "losses[t] = loss.item()\n",
    "if update_every(t, 1):\n",
    "    set_plot_data(y_pred.detach().numpy(), losses[:t+1])\n",
    "\n",
    "# zero out any previous gradients\n",
    "model.zero_grad()\n",
    "\n",
    "# compute gradient of loss w.r.t model parameters\n",
    "loss.backward()\n",
    "\n",
    "# update the parameters using SGD\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually go ahead and turn this into a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "n_training_steps = 1000\n",
    "losses = np.ones(n_training_steps) * np.infty\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model[0].reset_parameters()\n",
    "set_plot_data(model(xx).detach().numpy(), losses[:1])\n",
    "\n",
    "\n",
    "for t in range(n_training_steps):       \n",
    "    # run forward pass\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = loss_fn(y_pred, yy)\n",
    "\n",
    "    # update plots if needed\n",
    "    losses[t] = loss.item()\n",
    "    if update_every(t, 10):\n",
    "        set_plot_data(y_pred.detach().numpy(), losses[:t+1])\n",
    "\n",
    "    # zero out any previous gradients\n",
    "    model.zero_grad() \n",
    "    \n",
    "    # compute gradient of loss w.r.t model parameters\n",
    "    loss.backward() \n",
    "\n",
    "    # update parameters\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "set_plot_data(y_pred.detach().numpy(), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `IPython.display` to display a png copy of our figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Optimizers\n",
    "\n",
    "We can use optimizers to perform the parameter update step automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_steps = 200\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "losses = np.ones(n_training_steps) * np.infty\n",
    "\n",
    "#parameters = np.zeros((n_training_steps, 2))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model[0].reset_parameters()\n",
    "set_plot_data(model(xx).detach().numpy(), losses[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(n_training_steps):\n",
    "    #parameters[t] = [p.item() for p in model.parameters()]\n",
    "    \n",
    "    # run forward pass\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = loss_fn(y_pred, yy)\n",
    "\n",
    "    # update plots if needed\n",
    "    losses[t] = loss.item()\n",
    "    if update_every(t, 5):\n",
    "        set_plot_data(y_pred.detach().numpy(), losses[:t+1])\n",
    "\n",
    "    # zero out any previous gradients\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    # compute gradient of loss w.r.t model parameters\n",
    "    loss.backward() \n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "set_plot_data(y_pred.detach().numpy(), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a look at the model parameters again, we can verfiy that they now closely resemble our _target_ parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[param[1].item() for param in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a, b] # target parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization of the behaviour of different optimizers, check out [this](https://distill.pub/2017/momentum/), [this](https://bl.ocks.org/EmilienDupont/aaf429be5705b219aaaf8d691e27ca87) and [this](https://ikocabiyik.com/blog/en/visualizing-ml-optimizers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Weight Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed); # for reproducibility\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mesh = 200\n",
    "\n",
    "xv, yv = np.meshgrid(\n",
    "    np.linspace(a - 2, a + 2, n_mesh), \n",
    "    np.linspace(b - 2, b + 2, n_mesh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv.shape, yv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxv = torch.from_numpy(xv)\n",
    "yyv = torch.from_numpy(yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz = torch.zeros_like(xxv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (a_, b_) in enumerate(zip(xxv.flatten(), yyv.flatten())):\n",
    "        pa, pb = model.parameters()\n",
    "        pa.copy_(torch.tensor([[a_]]))\n",
    "        pb.copy_(torch.tensor([b_]))\n",
    "        loss = loss_fn(model(xx), yy)\n",
    "        zzz.flatten()[i] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, LogLocator, FormatStrFormatter\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Plot the surface.\n",
    "min_loss = zzz.min().item()\n",
    "max_loss = zzz.max().item()\n",
    "surf = ax.plot_surface(xxv.numpy(), yyv.numpy(), zzz.numpy(),\n",
    "                       cmap=cm.inferno,\n",
    "                       norm=mpl.colors.LogNorm(vmin=min_loss, vmax=max_loss),\n",
    "                       antialiased=False)\n",
    "\n",
    "idx_min = zzz.argmin()\n",
    "ax.scatter3D(xxv.flatten()[idx_min], yyv.flatten()[idx_min], zzz.flatten()[idx_min], c='r',  zorder=10)\n",
    "\n",
    "ax.set_xlabel('a')\n",
    "ax.set_ylabel('b')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "from matplotlib import ticker, cm\n",
    "\n",
    "# Automatic selection of levels works; setting the\n",
    "# log locator tells contourf to use a log scale:\n",
    "fig, ax = plt.subplots()\n",
    "cs = ax.contourf(xxv.numpy(), yyv.numpy(), zzz.numpy(), \n",
    "                \n",
    "                levels=np.logspace(np.log10(min_loss), np.log10(max_loss), 50),\n",
    "                 #levels=np.linspace(min_loss, max_loss, 50),\n",
    "                norm=mpl.colors.LogNorm(vmin=min_loss, vmax=max_loss),\n",
    "                 #locator=ticker.LogLocator(),\n",
    "                 cmap=cm.inferno,\n",
    "                )\n",
    "\n",
    "idx_min = zzz.argmin()\n",
    "ax.scatter(xxv.flatten()[idx_min], yyv.flatten()[idx_min], c='r')\n",
    "\n",
    "ax.set_xlabel('a')\n",
    "ax.set_ylabel('b')\n",
    "#cbar = fig.colorbar(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax.plot(parameters[:, 0], parameters[:, 1], linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Closer Look: Automatic differentiation with ```torch.autograd```\n",
    "\n",
    "Adapted from [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two tensors `a` and `b` with `requires_grad=True`. This signals to `autograd` that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([4.], requires_grad=True)\n",
    "b = torch.tensor([2.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another tensor ```y``` from ```a``` and ```b```:\n",
    "\n",
    "$ y = 3a^3 - b^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 3 * a**3 - b**2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume ```a``` and ```b``` to be parameters of a neural network, and ```y``` to be the error. In NN training, we want gradients of the error w.r.t the parameters, i.e.\n",
    "\n",
    "$ \\frac{\\partial y}{\\partial a} = 9a^2 $ and $\\frac{\\partial y}{\\partial b} = -2b $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call ```.backward()``` on ```y```, autograd calculates these gradients and stores them in the respective tensors' ```.grad``` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads = torch.tensor([1., 1., 1.])\n",
    "# y.backward(gradient=grads)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly pass a ```gradient``` argument in ```y.backward()``` if ```y``` is a vector. ```gradient``` is a tensor of the same shape as ```y```, and it represents the gradient of ```y``` w.r.t itself, i.e.\n",
    "\n",
    "$ \\frac{\\partial y}{\\partial y} = 1 $\n",
    "\n",
    "Note: the ```gradient``` argument only has to be specified if ```y``` is a non-scalar (i.e. its data has more than one element), otherwise it can be created implicitly.\n",
    "\n",
    "Think of the ```gradient``` argument as a boolean vector specifying for which elements of ```y``` we want the gradients to be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are now deposited in ```a.grad``` and ```b.grad```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that they are actually correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad == 9 * a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad == -2 * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
