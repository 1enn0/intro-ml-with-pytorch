{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PyTorch** (1/2): the world's simplest neural net from scratch\n",
    "\n",
    "In this notebook, we will build a simple neural network to learn a linear equation with one variable a.k.a. a _line_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch?\n",
    "* open-source machine learning library\n",
    "* developed by Facebook AI Research + community\n",
    "* used in most state-of-the-art research\n",
    "* _Python first_: deeply integrated into Python, it should _feel familiar_ to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Components\n",
    "* ```torch```: tensor library (like NumPy but with GPU support)\n",
    "* ```torch.autograd```: automatic differentiation library\n",
    "* ```torch.jit```: compile PyTorch code to TorchScript for deployment (e.g. in standalone C++ program)\n",
    "* ```torch.nn```: neural network library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Links\n",
    "* [ðŸ”— Official PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "Tutorials:\n",
    "* [ðŸ”— 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* [ðŸ”— PyTorch Tutorials with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network to Learn a Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the ML introduction, we need some data to train our model.\n",
    "\n",
    "In our case, we can just generate data points for an arbitrary line function. In order to do this we first need to define our _target_, i.e. the line equation that we want our model to learn. \n",
    "\n",
    "As you might recall, a _line_ is defined by the equation $$ y = ax + b $$\n",
    "\n",
    "It has one variable ($x$) and two parameters:\n",
    "* $a$ (_slope_) and \n",
    "* $b$ (_intercept_). \n",
    "\n",
    "The _parameters_ are what we want our model to learn.\n",
    "\n",
    "Let's create our target line function by generating some random values for $a$ and $b$.\n",
    "\n",
    "\n",
    "> When developing algorithms that contain any kind of randomness, for reproducibility it is always a good idea to set a _manual seed_ before you\n",
    "start. This will initialize the random number generators to the same state every time you run your experiment, i.e. you will always get the same results. \n",
    ">\n",
    "> In NumPy, you can do this by passing an arbitrary number to the function `numpy.random.seed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.random.uniform(low=-2.0, high=2.0, size=2)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use these to define our target function $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our _independent variable_ $x$ by generating 1000 evenly spaced numbers over the interval $[-\\pi, \\pi]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "x = np.linspace(-np.pi, np.pi, n_samples, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the number of samples we generate will influence our results: the more data points we allow the model to learn from, the better our final performance will be.\n",
    "\n",
    "Let's create our _dependent variable_ $y$ by passing $x$ to our function $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what our line looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y_true, color='#455a64', label='y_true')\n",
    "ax.grid(True, alpha=0.4)\n",
    "ax.legend()\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, you usually don't know _true_ process that you are trying to model. You only have some data obtained from observing the process.\n",
    "\n",
    "Any measured data will include some kind of noise (e.g. slight deviations in temperature readings due to a given precision of the sensor itself). Luckily, neural networks can handle noisy data quite well. Actually, introducing noise during training (especially for small datasets) can improve the robustness of the network and result in a better generalization (see e.g. [here](https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/)).\n",
    "\n",
    "So, let's go ahead and add some noise to our _true_ $y$ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.random.normal(size=y_true.shape, scale=0.5).astype(np.float32)\n",
    "y_noisy = y_true + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(x, y_noisy, s=1, color='#90a4ae', alpha=0.75, label='y_noisy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a neural network to learn this function, we need a model that also has (at least) two parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we need to decide what _kind_ of layers our neural network is made of. Since we want to model a linear equation, `torch.nn.Linear` might be a good fit. Let's have a look at [what this layer does](https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we need! Conceptually, it looks like this:\n",
    "\n",
    "<img src=\"../assets/images/linear_unit.svg\" alt=\"Linear unit\" width=\"400px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $n$ inputs $x_1, x_2, \\dots, x_n$, each of which is multiplied by its corresponding weight $w_1, w_2, \\dots, w_n$. The output $y$ is the weighted sum of all inputs plus a single bias term $b$.\n",
    "\n",
    "For our specific case, it looks even simpler:\n",
    "\n",
    "<img src=\"../assets/images/linear_unit_single.svg\" alt=\"Linear unit with a single input\" width=\"400px\"/>\n",
    "\n",
    "We just have a single input ($x$), the weight corresponds to the slope $a$ and the bias $b$ corresponds to the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct an instance of `torch.nn.Linear`, we need specify the sizes of our input and output samples. In our case, these are just scalar values (i.e. shape `1`).\n",
    "\n",
    "Again, we set a manual seed here because the layer object takes care of randomly initializing its parameters. The PyTorch way of doing this is by calling `torch.manual_seed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed); # for reproducibility\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add a `Flatten` layer at the end. This will flatten the output to a 1D tensor to match the shape of our labels (i.e. the `y_noisy` vector).\n",
    "\n",
    "We can have have a look at our model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(name, tensor.item()) for (name, tensor) in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have two scalar parameters: a _weight_ and a _bias_.\n",
    "\n",
    "Currently, our training data is stored in NumPy arrays. To be able pass the data to our model, we have to convert them to PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.from_numpy(x).unsqueeze(-1)\n",
    "yy = torch.from_numpy(y_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, our model parameters are initialized to random values. \n",
    "\n",
    "Let's have a look at what our model predicts with these intital parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, figsize=(6, 6), gridspec_kw={'height_ratios':[0.7, 0.3]})\n",
    "axes[0].scatter(x, y_noisy, s=0.5, color='#90a4ae', alpha=0.75, label='y_noisy')\n",
    "axes[0].plot(x, y_true, linewidth=1, label='y_true', color='#455a64')\n",
    "axes[0].set_title('Epoch 0')\n",
    "line_pred = axes[0].plot(x, model(xx).detach().numpy(), label='y_pred', color='#ffa000')[0]\n",
    "line_loss = axes[1].semilogy(np.arange(n_training_steps)[0], 1000, label='training loss')[0]\n",
    "axes[1].set_xlim([0, n_training_steps])\n",
    "axes[1].set_xlabel('# epoch')\n",
    "axes[1].set_ylabel('mse loss')\n",
    "for ax in axes:\n",
    "    ax.grid(True, which='both', alpha=0.25)\n",
    "    ax.legend()\n",
    "fig.canvas.header_visible = False\n",
    "fig.tight_layout()\n",
    "\n",
    "def update_every(iteration, every = 50):\n",
    "    return iteration % every == every - 1\n",
    "\n",
    "def set_plot_data(y_pred, loss):\n",
    "    line_pred.set_ydata(y_pred)\n",
    "    line_loss.set_data(np.arange(loss.shape[0]), loss)\n",
    "    axes[0].set_title(f'Epoch {loss.size - 1}: loss {loss[-1]:.5f}')\n",
    "    axes[1].set_xlim([0, n_training_steps])\n",
    "    axes[1].relim()\n",
    "    axes[1].autoscale_view()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of the <a href=\"#detailed_loop\">Detailed Training Loop Diagram</a> and compare what we have achieved so far:\n",
    "\n",
    "* [x] Collect input data (our tensor `xx`) and\n",
    "* [x] Corresponding label data (our tensor `yy`)\n",
    "* [x] Define a model architecture\n",
    "* [x] Initialize model parameters to random values\n",
    "* [x] Calculate predictions (our tensor `y_pred`) based on current model parameters and input data\n",
    "* [ ] Define a loss function to determine the performance\n",
    "* [ ] Update model parameters based on loss using automated process (Stochastic Gradient Descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update our model parameters, we need to define a suitable loss function. \n",
    "\n",
    "We want `y_pred` to exactly match `y_true`, so we need a mathematical way of figuring out how _similar_ our prediction is to our target line.\n",
    "\n",
    "We can use the _mean squared error (MSE)_ to achieve this. It is defined like this:\n",
    "\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum_{i = 1}^{n} (Y_i - \\hat{Y_i})^2$\n",
    "\n",
    "For every data sample $i$, we compute the squared difference of the target value $Y_i$ and the predicted value $\\hat{Y_i}$. The squared differences are then added up accross all sample and divided by the number of samples to yield a single scalar value, the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Furthermore, we need the automated process that will find values for our model parameters $w$ and $b$, such that the predictions closely resemble our target line. This is what the SGD algorithm does for us.\n",
    "\n",
    "Specifically, these are the steps that are required to make our model learn from its experience:\n",
    "\n",
    "<br>\n",
    "<img src=\"../assets/images/sgd.svg\" alt=\"Stochastic Gradient Descent process\" width=\"700px\"/>\n",
    "<a href=\"https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb\">(Image Source)</a>\n",
    "<br>\n",
    "\n",
    "1. *Initialize* the weights.\n",
    "1. For each input sample, use these weights to *predict* the output value.\n",
    "1. Based on these predictions, calculate how good the model is (its *loss*).\n",
    "1. Calculate the *gradient*, which measures for each weight, how changing that weight would change the loss\n",
    "1. *Step* (that is, change) all the weights based on that calculation.\n",
    "1. Go back to the step 2, and *repeat* the process.\n",
    "1. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don't want to wait any longer).\n",
    "\n",
    "Essentially, we want SGD to find the (_a_) minimum of our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlfromscratch.com/content/images/2019/12/gradient-descent-optimized--1-.gif\" alt=\"SGD\" width=\"600px\"/>\n",
    "\n",
    "Example from [here](https://mlfromscratch.com/optimizers-explained/#/) (animation originally from [3blue1brown](https://www.youtube.com/watch?v=IHZwWFHWa-w))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* overfitting/underfitting https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0175\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "losses = np.ones(n_training_steps) * np.infty\n",
    "\n",
    "params_and_loss = torch.zeros((n_training_steps, 3))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model[0].reset_parameters()\n",
    "set_plot_data(model(xx).detach().numpy(), losses[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(n_training_steps):\n",
    "    y_pred = model(xx) # run forward pass\n",
    "    \n",
    "    loss = loss_fn(y_pred, yy) # calculate loss\n",
    "\n",
    "    # record current parameter values and loss\n",
    "    params_and_loss[t] = torch.tensor([*[p.detach() for p in model.parameters()], loss.detach()])\n",
    "    \n",
    "    # update plots if needed\n",
    "    losses[t] = loss.item()\n",
    "    if update_every(t, 5):\n",
    "        set_plot_data(y_pred.detach().numpy(), losses[:t+1])\n",
    "\n",
    "    optimizer.zero_grad() # zero out any previous gradients\n",
    "    \n",
    "    loss.backward() # compute gradient of loss w.r.t model parameters\n",
    "    \n",
    "    optimizer.step() # update parameters\n",
    "    \n",
    "set_plot_data(y_pred.detach().numpy(), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a look at the model parameters again, we can verfiy that they now closely resemble our _target_ parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[param.item() for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a, b] # target parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Weight Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mesh = 50\n",
    "\n",
    "xv, yv = torch.meshgrid(torch.linspace(a - 2, a + 2, n_mesh), torch.linspace(b - 2, b + 2, n_mesh))\n",
    "zv = torch.zeros_like(xv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(zip(xv.flatten(), yv.flatten())):\n",
    "        pa, pb = model.parameters()\n",
    "        pa.copy_(x.reshape(pa.shape))\n",
    "        pb.copy_(y.reshape(pb.shape))\n",
    "        loss = loss_fn(model(xx), yy)\n",
    "        zv.flatten()[i] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "min_loss = zv.min().item()\n",
    "max_loss = zv.max().item()\n",
    "ax.plot_surface(\n",
    "    xv.numpy(), yv.numpy(), zv.numpy(),\n",
    "    cmap=mpl.cm.inferno,\n",
    "    norm=mpl.colors.LogNorm(vmin=min_loss, vmax=max_loss * 2),\n",
    "    antialiased=False)\n",
    "ax.plot(*params_and_loss.T, zorder=20, color='white')\n",
    "\n",
    "ax.set_xlabel('a')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('MSE')\n",
    "ax.view_init(elev=30, azim=-110)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cs = ax.contourf(\n",
    "    xv.numpy(), yv.numpy(), zv.numpy(), \n",
    "    levels=np.logspace(np.log10(min_loss), np.log10(max_loss), n_mesh),\n",
    "    norm=mpl.colors.LogNorm(vmin=min_loss, vmax=max_loss * 2),\n",
    "    cmap=mpl.cm.inferno)\n",
    "ax.plot(*params_and_loss.T[:2], color='white', label='model params')\n",
    "ax.scatter(a, b, color='red', label='target')\n",
    "ax.text(*(params_and_loss[0, :2] + 0.01), 'initial', color='white')\n",
    "ax.text(*(params_and_loss[-1, :2] - 0.01), 'final', color='white', ha='right')\n",
    "\n",
    "ax.set_xlabel('a')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_title('Model parameter trajectory during training')\n",
    "ax.legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Closer Look: Automatic differentiation with ```torch.autograd```\n",
    "\n",
    "`torch.autograd` is PyTorchâ€™s automatic differentiation engine that powers neural network training.\n",
    "\n",
    "Adapted from [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s take a look at how `autograd` collects gradients. We create two tensors `a` and `b` with `requires_grad=True`. This signals to `autograd` that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([4.], requires_grad=True)\n",
    "b = torch.tensor([2.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another tensor ```y``` from ```a``` and ```b```:\n",
    "\n",
    "$ y = 2a - b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * a - b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume ```a``` and ```b``` to be parameters of a neural network, and ```y``` to be the error. In NN training, we want gradients of the error w.r.t the parameters, i.e.\n",
    "\n",
    "$ \\frac{\\partial y}{\\partial a} = 2 $ and $\\frac{\\partial y}{\\partial b} = -1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call ```.backward()``` on ```y```, autograd calculates these gradients and stores them in the respective tensors' ```.grad``` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are now deposited in ```a.grad``` and ```b.grad```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad, b.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
