{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PyTorch** (2/2): Audio Classification\n",
    "\n",
    "In this tutorial, we will train a neural network for audio classification using transfer learning. \n",
    "\n",
    "Some of the code in this example is based on [this](https://github.com/hasithsura/Environmental-Sound-Classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from helpers.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device to run on\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Dataset\n",
    "\n",
    "We will use the [ESC-50](https://github.com/karolpiczak/ESC-50) (Dataset for Environmental Sound Classification) in our experiment. If you cloned this repository including the `--recurse-submodules` flag, it is already downloaded and should be present at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes a `.csv` file that contains information about the samples in the dataset (e.g. which category it belongs to, etc.). We use `pandas` to read the `.csv` into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_ROOT/'meta/esc50.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first few lines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, in the docs of the dataset it states\n",
    "\n",
    "> The dataset has been prearranged into 5 folds for comparable cross-validation, making sure that fragments from the same original source file are contained in a single fold.\n",
    "\n",
    "This means, that we can just pick one of the folds to use as our validation set. We create a new column `is_valid` that is true for fold 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_valid'] = df['fold'] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase\n",
    "\n",
    "Instead of training a model entirely from scratch, we will use the concept of [_transfer learning_](https://en.wikipedia.org/wiki/Transfer_learning). This means we start with a pretrained model from a different but related problem and _fine tune_ it to our specific problem.\n",
    "\n",
    "Many of the break-through achievements in deep learning are coming from the domain of computer vision (e.g. image classification). By transforming audio into images (spectrograms) we can use those very same networks to perform audio classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Input Transformation Pipeline\n",
    "\n",
    "A fast.ai [`DataBlock`](https://docs.fast.ai/data.block.html#DataBlock) is a convenient way of organizing our data into a form that can be used during the training phase. \n",
    "\n",
    "> By itself, a DataBlock is just a blue print on how to assemble your data. It does not do anything until you pass it a source. You can choose to then convert that source into a Datasets or a DataLoaders by using the DataBlock.datasets or DataBlock.dataloaders method.\n",
    "\n",
    "If you want to know more about DataBlocks, have a look at this [tutorial](https://docs.fast.ai/tutorial.datablock.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio2spec = AudioToSpec.from_cfg(AudioConfig.BasicMelSpectrogram(sample_rate=16000, n_fft=2048, hop_length=512, n_mels=128))\n",
    "normalize = AudioNormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = DataBlock(\n",
    "    blocks=(AudioBlock, CategoryBlock),\n",
    "    splitter = ColSplitter(),\n",
    "    get_x = ColReader('filename', pref=DATASET_ROOT/'audio'),\n",
    "    get_y = ColReader('category'),\n",
    "    item_tfms=[normalize],\n",
    "    batch_tfms=[audio2spec],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = block.dataloaders(df, num_workers=0) # on windows, num_workers needs to be set to 0\n",
    "# dls.show_batch(figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create/Prepare Model\n",
    "\n",
    "Now, we create a `learner` object by passing it our DataLoaders object and using a pretrained ResNet18 as our architechture.\n",
    "\n",
    "If you run this the first time, it will download the pretrained ResNet18 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, models.resnet18, metrics=[accuracy], pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResNet models work on RGB images, i.e. the input layer expects an image with three channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model[0][0].in_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our spectrograms only have a single channel, so we need to modify the first layer of the model slightly to take our spectrograms as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_xresnet_grayscale(learn.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CUDA is available, we now transfer our model to the GPU. (If you do not have CUDA available, this cell will do nothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "... and finally we call `fine_tune` to train the model for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Training Results\n",
    "\n",
    "After 5 epochs of training, you should see an accuracy of ~65 %. This may not seem very impressive at first but keep in mind that we just trained the model for a couple of minutes. Also, we are working with 50 output classes! \n",
    "\n",
    "If you have a look at the [original experiment](https://github.com/hasithsura/Environmental-Sound-Classification), you can see that with some further training it is easy to achieve an accuracy of over 85 %.\n",
    "\n",
    "To get a better understanding of how well our model is doing, we can create a `ClassificationInterpretation` object from our learner and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can take a look at which pairs of categories the model has the most trouble with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused(min_val=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Trained Model\n",
    "\n",
    "For running inference in our C++ application, we need to do two things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Model as TorchScript\n",
    "\n",
    "[TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#) is a different representation of a PyTorch model that can be loaded and run in C++. The process of transforming our trained PyTorch model into a TorchScript module is called _tracing_. \n",
    "\n",
    "Tracing a model will invoke it with some _example input_ and record all the operations that occur during its execution. These recorded operations will then be saved into a static representation of our model (called _graph_).\n",
    "\n",
    "In our case, all we need to do is call `torch.jit.trace()`, passing our model and some dummy input as arguments. As dummy input, we will use a random tensor that has the same shape as our spectrograms during training (`[1, 128, 157]`, i.e. `[n_channels, n_mels, n_frames]`), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model back to CPU memory if we were training on GPU\n",
    "if use_cuda:\n",
    "    learn.model.to('cpu')\n",
    "    \n",
    "# create some dummy input \n",
    "dummy_input = torch.randn([1, 1, 128, 157])\n",
    "    \n",
    "# process the trace\n",
    "traced_script_module = torch.jit.trace(learn.model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dir if it doesn't exist\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(traced_script_module, str(MODEL_DIR/'esc-model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Vocabulary\n",
    "\n",
    "Our model does not directly output a string with the predicted category. Rather, it outputs a vector containing an unnormalized score for each of the categories in our vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model(torch.randn(1, 1, 128, 157))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are oredered in the same way as our vocabulary, so converting the model output to the string of the predicted category is straightforward: we just need to find the index of the item with the highest score (using `argmax()`) and use this as an index into our vocabulary vector.\n",
    "\n",
    "To be able to perform this mapping directly in out C++ application, we our vocabulary into a header file that will automatically be placed in the C++ source directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_cpp_header(dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure, we will also export the `vocab` list as a [pickle](https://docs.python.org/3/library/pickle.html) file, so we can easily reuse in some other Python script if we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_DIR/'esc-model-vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(dls.vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Test Dataset\n",
    "\n",
    "The `DataLoader` class we used earlier automatically takes care of splitting the dataset into training and validation sets. For testing purposes, however, we cannot use samples from either of these. This leaves us with two options:\n",
    "\n",
    "1. splitting off some items of the dataset for testing before creating the `DataLoader` for training or\n",
    "2. using new samples, e.g. from a different dataset.\n",
    "\n",
    "Since we are already done with training, we will go for the latter option. I have manually selected some sound examples from [freesound.org](https://freesound.org/):\n",
    "\n",
    "* [an airplane](https://freesound.org/people/AurelioSons/sounds/207457/),\n",
    "* [a guy sneezing](https://freesound.org/people/InspectorJ/sounds/368804/),\n",
    "* [a toilet flush](https://freesound.org/people/InspectorJ/sounds/404329/),\n",
    "* [a dog bark](https://freesound.org/people/Juan_Merie_Venter/sounds/327666/),\n",
    "* [some sheeps](https://freesound.org/people/zachrau/sounds/362283/),\n",
    "* [a child laughing](https://freesound.org/people/Teumova/sounds/439667/),\n",
    "* [some mouse clicking](https://freesound.org/people/Masgame/sounds/347544/),\n",
    "* [a siren](https://freesound.org/people/Kingrock2009/sounds/544376/),\n",
    "* [a coffee machine](https://freesound.org/people/Acekat13/sounds/515685/),\n",
    "* [a harp](https://freesound.org/people/pryght%20one/sounds/27130/) and\n",
    "* [a synth sound](https://freesound.org/people/Erokia/sounds/550708/).\n",
    "\n",
    "Excecpt for last three examples, all samples belong to categories known to our model.\n",
    "\n",
    "To begin with, we need to manually label our test data, i.e. we create a dictionary that maps filenames to their category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_to_label = {\n",
    "    '404329__inspectorj__toilet-flush-european-distant-lid-up.wav': 'toilet_flush',\n",
    "    '327666__juan-merie-venter__dog-bark.wav': 'dog',\n",
    "    '368804__inspectorj__sneeze-single-b.wav': 'sneezing',\n",
    "    '347544__masgame__mouse-click-sounds.wav': 'mouse_click',\n",
    "    '207457__aureliosons__avion-de-elices.wav': 'airplane',\n",
    "    '27130__pryght-one__harp.wav': 'harp (unknown category)',\n",
    "    '439667__teumova__child-laughing.wav': 'laughing',\n",
    "    '515685__acekat13__adriana-lopez-coffee-machine.wav': 'coffee_machine (unknown category)',\n",
    "    '544376__kingrock2009__siren-1.wav': 'siren',\n",
    "    '362283__zachrau__sheep-bleating.wav': 'sheep',\n",
    "    '550708__erokia__msfxp9-14-synth-loop-100-bpm.wav': 'synth_loop (unknown category)'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model expects spectrograms, not the raw waveforms. During training, this transformation was automatically handled by the `DataLoader` class. \n",
    "\n",
    "We don't need all the functionality offered by the `DataLoader`, we just want to transform all test samples. For this, we can use the `Pipeline` class. As the name suggests, this is just a pipeline of `Transform`s.\n",
    "\n",
    "Because we are not dealing with a homogenous dataset as during training, we need to add a couple of transforms to make sure that our test samples have the same properties as those from the training dataset:\n",
    "* `Resample`: resample to 16 kHz\n",
    "* `DownmixMono`: downmix stereo signals to mono\n",
    "* `ResizeSignal`: resize to exactly 5 s in length (padding or clipping if needed)\n",
    "\n",
    "Let's define our transform pipeline, using the `normalize`, `audio2spec` and `gray2rgb` transforms we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_to_5s = ResizeSignal(5000, pad_mode=AudioPadType.Zeros_After)\n",
    "resample_to_16khz = Resample(16000)\n",
    "downmix = DownmixMono()\n",
    "\n",
    "transforms = Pipeline([AudioTensor.create, resize_to_5s, downmix, resample_to_16khz, normalize, audio2spec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a new dictionary mapping our the label of each test sample to the output of the `transforms` pipeline (i.e. our spectrograms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_spectrogram = {label: transforms(TEST_DIR/'data'/filename) for filename, label in test_files_to_label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the spectrograms into a TorchScript module to be able to load it from the C++ application.\n",
    "\n",
    "To have access to the labels from within the C++ application, we register each of our spectrograms as a _named buffer_, using the corresponding label as name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test samples as named buffers in a module\n",
    "container = torch.nn.Module()\n",
    "for l, s in label_to_spectrogram.items():\n",
    "    container.register_buffer(l, torch.tensor(s))\n",
    "    \n",
    "# save to torch script module\n",
    "torch.jit.save(torch.jit.script(container), str(TEST_DIR/'inputs.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the C++ application\n",
    "\n",
    "For demo purposes, we will be running the inference part in C++.\n",
    "\n",
    "In IPython (which is running in the backend kernel of this notebook), we can run any command-line command by prefixing it with a `!`. \n",
    "\n",
    "This way we can just build and call the C++ application from right here within the notebook! We can also reuse any of our currently defined variables by including them in `{`curly braces`}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for `cmake`\n",
    "\n",
    "Make sure you have `cmake` installed and its executable can be found. If everything is set up correctly, you should see the version output from `cmake` when executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Build System\n",
    "\n",
    "After making sure that our build directory exists, we call `cmake` to generate our build system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create build directory if it does not exist\n",
    "if not CPP_BUILD_DIR.exists():\n",
    "    CPP_BUILD_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPP_SOURCE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake -S {CPP_SOURCE_DIR} -B {CPP_BUILD_DIR} -DCMAKE_PREFIX_PATH={torch.utils.cmake_prefix_path}\n",
    "\n",
    "# if you're running cmake < 3.14, comment out the previous command and run the following instead\n",
    "# !pushd {CPP_BUILD_DIR} && cmake .. -DCMAKE_PREFIX_PATH={torch.utils.cmake_prefix_path} && popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Application\n",
    "\n",
    "Next, we can trigger the actual build.\n",
    "\n",
    "**WARNING**: note that you cannot just switch the build config to `Debug`, as the `libtorch` libraries for `Debug`/`Release` are not ABI-compatible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build {CPP_BUILD_DIR} --config Release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "\n",
    "After building the application successfully, we can call it to run inference using our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esc_executable = CPP_BUILD_DIR/'esc-app'\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    esc_executable = ((esc_executable.parent/'Release')/esc_executable.name).with_suffix('.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{esc_executable}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODEL_DIR/'esc-model.pt'\n",
    "inputs_path = TEST_DIR/'inputs.pt'\n",
    "outputs_path = TEST_DIR/'outputs.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{esc_executable} {model_path} {inputs_path} {outputs_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `ipywidgets` to interactively explore our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = next(torch.load(outputs_path).parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output contains unnormalized scores we use `softmax` to convert them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_prob = torch.softmax(outputs.detach(), dim=1)\n",
    "outputs_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_prob[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `ipywidgets`, we can create GUI widgets to interactively explore our results. At first, we define a dropdown menu that contains the labels from our test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = list(test_files_to_label.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown = widgets.Dropdown(options = test_labels, description='Test sample: ')\n",
    "dropdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on our selection in this dropdown, we want print the output probabilities from our model. We can do this by using the `Output` widget. It acts as a context manager, capturing all output that is produced during the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, however, this is just an empty placeholder. \n",
    "\n",
    "We need to create an event handler that calls the `print_top_results` using the active index from the dropdown menu and register it as an observer of the `index` trait on the dropdown menu object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_dropdown(_):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        print_top_results(dropdown.index, outputs_prob, test_labels, dls.vocab)\n",
    "    \n",
    "dropdown.observe(on_dropdown, names=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select some items from the Dropdown menu!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine these widgets into Layouts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_plot = widgets.Output()\n",
    "def on_dropdown(_):\n",
    "    output_plot.clear_output()\n",
    "    with output_plot:\n",
    "        fig, _ = plot_top_results(outputs_prob[dropdown.index], dls.vocab, show_max=10, figsize=(5,3))\n",
    "        fig.canvas.toolbar_visible = False\n",
    "        fig.tight_layout()\n",
    "dropdown.observe(on_dropdown, names=['index'])\n",
    "\n",
    "widgets.VBox([dropdown, output_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Example\n",
    "\n",
    "Using our trained model and `ipywidgets`, we can even build a small application right here from inside the notebook (inspired an example from the [fast.ai course](https://course.fast.ai/videos/?lesson=3https://course.fast.ai/videos/?lesson=3)).\n",
    "\n",
    "Select \n",
    "\n",
    "Note: selecting an audio file longer than 5 s and classifying it multiple times might yield different results due to a random cropping in the `ResizeSignal` transform.Note: selecting an audio file longer than 5 s and classifying it multiple times might yield different results due to a random cropping in the `ResizeSignal` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_DIR/'esc-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload = widgets.FileUpload(accept='.wav')\n",
    "btn_classify = widgets.Button(description='Classify', layout=widgets.Layout(width='300px'))\n",
    "audio_player = widgets.Audio(autoplay=False, loop=False, layout=widgets.Layout(width='300px'))\n",
    "label_upload = widgets.Label(value='No file selected')\n",
    "label_class = widgets.Label(layout=widgets.Layout(width='300px'))\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_click_upload(_):\n",
    "    audio_player.value = btn_upload.data[-1]\n",
    "    label_upload.value = btn_upload.metadata[-1]['name']\n",
    "    btn_classify.description = 'Classify'\n",
    "    \n",
    "def classify(_):\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False)\n",
    "    tmp.file.write(btn_upload.data[-1]); tmp.close()\n",
    "\n",
    "    # load tensor from tmp file and apply transforms\n",
    "    spec = torch.tensor(transforms(tmp.name))\n",
    "    probs = model(spec.unsqueeze(0)).softmax(dim=1).squeeze().detach()\n",
    "    result_idx = probs.argmax()\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        fig, _ = plot_top_results(probs, dls.vocab, show_max=10, figsize=(5, 3))\n",
    "        fig.canvas.header_visible = False\n",
    "        fig.canvas.toolbar_visible = False\n",
    "        fig.tight_layout()\n",
    "\n",
    "    tmp.close(); os.unlink(tmp.name)\n",
    "\n",
    "def reset_button(_):\n",
    "    btn_upload.value.clear()\n",
    "    btn_upload._counter = 1\n",
    "\n",
    "btn_upload.observe(on_click_upload, names=['data'])\n",
    "btn_classify.on_click(classify)\n",
    "btn_upload.observe(reset_button, names=['value'])\n",
    "\n",
    "widgets.VBox([widgets.HBox([btn_upload, label_upload]), audio_player, btn_classify, output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have successfully \n",
    "\n",
    "* trained a neural network to classify audio examples into 50 categories\n",
    "* built a C++ application to load and run the trained model\n",
    "* created small interactive applications to analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
