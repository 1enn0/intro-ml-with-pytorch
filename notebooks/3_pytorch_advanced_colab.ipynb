{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/\"><img src=\"https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png\" width=\"600px\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PyTorch** (2/2): Audio Classification\n",
    "\n",
    "In this tutorial, we will train a neural network for audio classification using transfer learning. \n",
    "\n",
    "Some of the code in this example is based on [this](https://github.com/hasithsura/Environmental-Sound-Classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab-specific Setup\n",
    "\n",
    "#### Change runtime to GPU\n",
    "Click on `Runtime` > `Change runtime type`, select GPU in the dropdown and click save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install missing packages\n",
    "\n",
    "We need to install some packages that are not included in the standard Colab environment.\n",
    "\n",
    "You will get some errors about incompatible versions of `jupyter-console` and `google-colab`. You can ignore these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq fastai==2.1.9\n",
    "!pip install -Uq fastaudio==0.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use the new packages, you have to restart the runtime by clicking `Runtime` > `Restart Runtime`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone repo for supplemental files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recurse-submodules https://github.com/1enn0/intro-ml-with-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./intro-ml-with-pytorch/notebooks')\n",
    "\n",
    "from helpers.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device to run on\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Dataset\n",
    "\n",
    "We will use the [ESC-50](https://github.com/karolpiczak/ESC-50) (Dataset for Environmental Sound Classification) in our experiment. If you cloned this repository including the `--recurse-submodules` flag, it is already downloaded and should be present at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes a `.csv` file that contains information about the samples in the dataset (e.g. which category it belongs to, etc.). We use `pandas` to read the `.csv` into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_ROOT/'meta/esc50.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first few lines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we are not [overfitting](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76), we have to set aside part of our dataset as a [validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets).\n",
    "\n",
    "Fortunately, in the docs of the dataset it states\n",
    "\n",
    "> The dataset has been prearranged into 5 folds for comparable cross-validation, making sure that fragments from the same original source file are contained in a single fold.\n",
    "\n",
    "This means, that we can just pick one of the folds to use as our validation set. We create a new column `is_valid` that is true for fold 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_valid'] = df['fold'] == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase\n",
    "\n",
    "Instead of training a model entirely from scratch, we will use the concept of [_transfer learning_](https://en.wikipedia.org/wiki/Transfer_learning). This means we start with a pretrained model from a different but related problem and _fine tune_ it to our specific problem.\n",
    "\n",
    "Many of the break-through achievements in deep learning are coming from the domain of computer vision (e.g. image classification). By transforming audio into images (spectrograms) we can use those very same networks to perform audio classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Input Transformation Pipeline\n",
    "\n",
    "A fast.ai [`DataBlock`](https://docs.fast.ai/data.block.html#DataBlock) is a convenient way of organizing our data into a form that can be used during the training phase. \n",
    "\n",
    "> By itself, a DataBlock is just a blue print on how to assemble your data. It does not do anything until you pass it a source. You can choose to then convert that source into a Datasets or a DataLoaders by using the DataBlock.datasets or DataBlock.dataloaders method.\n",
    "\n",
    "If you want to know more about DataBlocks, have a look at this [tutorial](https://docs.fast.ai/tutorial.datablock.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio2spec = AudioToSpec.from_cfg(AudioConfig.BasicMelSpectrogram(sample_rate=16000, n_fft=2048, hop_length=512, n_mels=128))\n",
    "normalize = AudioNormalize()\n",
    "gray2rgb = SpectrogramToFakeRGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = DataBlock(\n",
    "    blocks=(AudioBlock, CategoryBlock),\n",
    "    splitter = ColSplitter(),\n",
    "    get_x = ColReader('filename', pref=DATASET_ROOT/'audio'),\n",
    "    get_y = ColReader('category'),\n",
    "    item_tfms=[normalize],\n",
    "    batch_tfms=[audio2spec, gray2rgb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = block.dataloaders(df)\n",
    "# dls.show_batch(figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create/Prepare Model\n",
    "\n",
    "Now, we create a `Learner` object by passing it our `DataLoaders` object and using a pretrained ResNet18 as our architechture. By setting `pretrained=True`, we can automatically download a set of pretrained weights for the ResNet18 architecture. Additionally, this function will delete the last layer of the pretrained weights and and create a new randomly initialized one to match the number of output classes in out dataset. This is a standard procedure in transfer learning and is also referred to as replacing the _head_ of the model. \n",
    "\n",
    "You also have to specify the metric that you want to use. In our case, as in most classification tasks, we want our model to achieve a high accuracy, which is defined as\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, models.resnet18, metrics=[accuracy], pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CUDA is available, we now transfer our model to the GPU. (If you do not have CUDA available, this cell will do nothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "One particular challenge when training a model is to find a good learning rate. If the learning rate is very small, the training will be very slow and we might run into problems with overfitting. Choosing a very large learning rate is not a good option either because we will likely just overshoot the minimum loss.\n",
    "\n",
    "FastAI includes a [_learning rate finder_](https://docs.fast.ai/callback.schedule.html#Learner.lr_find) to automatically determine a good learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate, _ = learn.lr_find(show_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start the training by calling `fine_tune`. This is another FastAI convenience function that does the following: \n",
    "* first, it trains only the _head_ of the model (this is the new, untrained part) for one epoch and\n",
    "* then, it trains the whole model for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(5, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Training Results\n",
    "\n",
    "After 5 epochs of training, you should see an accuracy of ~70 %. This may not seem very good but keep in mind that we just trained the model for a couple of minutes. Also, we are working with 50 output classes! \n",
    "\n",
    "If you have a look at the [original experiment](https://github.com/hasithsura/Environmental-Sound-Classification), you can see that with some further training it is easy to achieve an accuracy of over 85 %.\n",
    "\n",
    "To get a better understanding of how well our model is doing, we can create a `ClassificationInterpretation` object from our learner and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can take a look at which pairs of categories the model has the most trouble with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused(min_val=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Trained Model\n",
    "\n",
    "For running inference in our C++ application, we need to do two things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Model as TorchScript\n",
    "\n",
    "[TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#) is a different representation of a PyTorch model that can be loaded and run in C++. The process of transforming our trained PyTorch model into a TorchScript module is called _tracing_. \n",
    "\n",
    "Tracing a model will invoke it with some _example input_ and record all the operations that occur during its execution. These recorded operations will then be saved into a static representation of our model (called _graph_).\n",
    "\n",
    "In our case, all we need to do is call `torch.jit.trace()`, passing our model and some dummy input as arguments. As dummy input, we will use a random tensor that has the same shape as our spectrograms during training (`[1, 128, 157]`, i.e. `[n_channels, n_mels, n_frames]`), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model back to CPU memory if we were training on GPU\n",
    "if use_cuda:\n",
    "    learn.model.to('cpu')\n",
    "    \n",
    "# create some dummy input \n",
    "dummy_input = torch.randn([1, 3, 128, 157])\n",
    "    \n",
    "# process the trace\n",
    "traced_script_module = torch.jit.trace(learn.model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dir if it doesn't exist\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(traced_script_module, str(MODEL_DIR/'esc-model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model does not directly output a string with the predicted category (more on this later). To be able to map the model output to the winning category string in the C++ application, we need to have a list of all categories. Our `DataLoaders` objects holds such a list as its `vocab` attribute. We write this list into header file and place it in the C++ source directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_cpp_header(dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Test Dataset\n",
    "\n",
    "The `DataLoader` class we used earlier automatically takes care of splitting the dataset into training and validation sets. For testing purposes, however, we cannot use samples from either of these. This leaves us with two options:\n",
    "\n",
    "1. splitting off some items of the dataset for testing before creating the `DataLoader` for training or\n",
    "2. using new samples, e.g. from a different dataset.\n",
    "\n",
    "Since we are already done with training, we will go for the latter option. I have manually selected some sound examples from [freesound.org](https://freesound.org/):\n",
    "\n",
    "* [an airplane](https://freesound.org/people/AurelioSons/sounds/207457/),\n",
    "* [a guy sneezing](https://freesound.org/people/InspectorJ/sounds/368804/),\n",
    "* [a toilet flush](https://freesound.org/people/InspectorJ/sounds/404329/),\n",
    "* [a dog bark](https://freesound.org/people/Juan_Merie_Venter/sounds/327666/),\n",
    "* [some sheeps](https://freesound.org/people/zachrau/sounds/362283/),\n",
    "* [a child laughing](https://freesound.org/people/Teumova/sounds/439667/),\n",
    "* [some mouse clicking](https://freesound.org/people/Masgame/sounds/347544/),\n",
    "* [a siren](https://freesound.org/people/Kingrock2009/sounds/544376/),\n",
    "* [a coffee machine](https://freesound.org/people/Acekat13/sounds/515685/),\n",
    "* [a harp](https://freesound.org/people/pryght%20one/sounds/27130/) and\n",
    "* [a synth sound](https://freesound.org/people/Erokia/sounds/550708/).\n",
    "\n",
    "Excecpt for last three examples, all samples belong to categories known to our model.\n",
    "\n",
    "To begin with, we need to manually label our test data, i.e. we create a dictionary that maps filenames to their category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_to_label = {\n",
    "    '404329__inspectorj__toilet-flush-european-distant-lid-up.wav': 'toilet_flush',\n",
    "    '327666__juan-merie-venter__dog-bark.wav': 'dog',\n",
    "    '368804__inspectorj__sneeze-single-b.wav': 'sneezing',\n",
    "    '347544__masgame__mouse-click-sounds.wav': 'mouse_click',\n",
    "    '207457__aureliosons__avion-de-elices.wav': 'airplane',\n",
    "    '439667__teumova__child-laughing.wav': 'laughing',\n",
    "    '544376__kingrock2009__siren-1.wav': 'siren',\n",
    "    '362283__zachrau__sheep-bleating.wav': 'sheep',\n",
    "    '515685__acekat13__adriana-lopez-coffee-machine.wav': 'coffee_machine (unknown)',\n",
    "    '27130__pryght-one__harp.wav': 'harp (unknown)',\n",
    "    '550708__erokia__msfxp9-14-synth-loop-100-bpm.wav': 'synth_loop (unknown)'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model expects spectrograms, not the raw waveforms. During training, this transformation was automatically handled by the `DataLoader` class. \n",
    "\n",
    "We don't need all the functionality offered by the `DataLoader`, we just want to transform all test samples. For this, we can use the `Pipeline` class. As the name suggests, this is just a pipeline of `Transform`s.\n",
    "\n",
    "Because we are not dealing with a homogenous dataset as during training, we need to add a couple of transforms to make sure that our test samples have the same properties as those from the training dataset:\n",
    "* `Resample`: resample to 16 kHz\n",
    "* `DownmixMono`: downmix stereo signals to mono\n",
    "* `ResizeSignal`: resize to exactly 5 s in length (padding or clipping if needed)\n",
    "\n",
    "Let's define our transform pipeline, using the `normalize` and `audio2spec` transforms we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_to_5s = ResizeSignal(5000, pad_mode=AudioPadType.Zeros_After)\n",
    "resample_to_16khz = Resample(16000)\n",
    "downmix = DownmixMono()\n",
    "gray2rgb_single = SpectrogramToFakeRGB(as_batch_tfm=False)\n",
    "\n",
    "transforms = Pipeline([AudioTensor.create, resize_to_5s, downmix, resample_to_16khz, normalize, audio2spec, gray2rgb_single])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a new dictionary mapping our the label of each test sample to the output of the `transforms` pipeline (i.e. our spectrograms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_spectrogram = {label: transforms(TEST_DIR/'data'/filename) for filename, label in test_files_to_label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the spectrograms into a TorchScript module to be able to load it from the C++ application.\n",
    "\n",
    "To have access to the labels from within the C++ application, we register each of our spectrograms as a _named buffer_, using the corresponding label as name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test samples as named buffers in a module\n",
    "container = torch.nn.Module()\n",
    "for l, s in label_to_spectrogram.items():\n",
    "    container.register_buffer(l, torch.tensor(s))\n",
    "    \n",
    "# save to torch script module\n",
    "torch.jit.save(torch.jit.script(container), str(TEST_DIR/'inputs.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the C++ application\n",
    "\n",
    "For demo purposes, we will be running the inference part in C++.\n",
    "\n",
    "In IPython (which is running in the backend kernel of this notebook), we can run any command-line command by prefixing it with a `!`. \n",
    "\n",
    "This way we can just build and call the C++ application from right here within the notebook! We can also reuse any of our currently defined variables by including them in `{`curly braces`}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for `cmake`\n",
    "\n",
    "Make sure you have `cmake` installed and its executable can be found. If everything is set up correctly, you should see the version output from `cmake` when executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Build System\n",
    "\n",
    "After making sure that our build directory exists, we call `cmake` to generate our build system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create build directory if it does not exist\n",
    "if not CPP_BUILD_DIR.exists():\n",
    "    CPP_BUILD_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pushd {CPP_BUILD_DIR} && cmake .. -DCMAKE_PREFIX_PATH={torch.utils.cmake_prefix_path} && popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Application\n",
    "\n",
    "Next, we can trigger the actual build.\n",
    "\n",
    "**WARNING**: note that you cannot just switch the build config to `Debug`, as the `libtorch` libraries for `Debug`/`Release` are not ABI-compatible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --build {CPP_BUILD_DIR} --config Release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "\n",
    "After building the application successfully, we can call it to run inference using our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esc_executable = CPP_BUILD_DIR/'esc-app'\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    esc_executable = ((esc_executable.parent/'Release')/esc_executable.name).with_suffix('.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{esc_executable}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODEL_DIR/'esc-model.pt'\n",
    "inputs_path = TEST_DIR/'inputs.pt'\n",
    "outputs_path = TEST_DIR/'outputs.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{esc_executable} {model_path} {inputs_path} {outputs_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exploration of the Results in Python\n",
    "\n",
    "In the C++ application, we saved the result of the inference to disk. We can load it again from Python using `torch.load()` to explore it a little more interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = next(torch.load(outputs_path).parameters()).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a tensor of shape `[n_samples, n_categories]`. Each row contains the output of one of the samples from our little test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first row, i.e. the first test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row contains an unnormalized score for each of our categories. To identify the categories we can have a look at the `vocab` attribute of our `DataLoaders` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores correspond directly to the categories in the `vocab` list, i.e. \n",
    "* the first item is the score of the `airplane` category,\n",
    "* the second item is the score of the `breathing` category,\n",
    "\n",
    "and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our classification result, we simply pick the item with the highest score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = outputs[0].max()\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in order to display the actual category string from the `vocab`, we need to find the index of the item returned by `max()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = outputs[0].argmax()\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test sample 0 is classified as: \\'{dls.vocab[idx]}\\' (score={score:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of working with unnormalized scores, it would be nice to display probabilities (s.t. the sum of each row equals 1). The `softmax` function does exactly this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_prob = outputs.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = outputs_prob[0].max()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test sample 0 is classified as: \\'{dls.vocab[idx]}\\' (p={p:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it Interactive with `ipywidgets`\n",
    "\n",
    "Using `ipywidgets`, we can create GUI widgets to interactively explore our results. At first, we define a dropdown menu that contains the labels from our test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = list(test_files_to_label.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown = widgets.Dropdown(options = test_labels, description='Test sample: ')\n",
    "dropdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on our selection in this dropdown, we want print the output probabilities from our model. We can do this by using the `Output` widget. It acts as a context manager, capturing all output that is produced during the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, however, this is just an empty placeholder. \n",
    "\n",
    "We need to create an event handler that calls the `print_top_results` using the active index from the dropdown menu and register it as an observer of the `index` trait on the dropdown menu object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_dropdown(change):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        print_top_results(change['new'], outputs_prob, test_labels, dls.vocab)\n",
    "    \n",
    "dropdown.observe(on_dropdown, names=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select some items from the Dropdown menu!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine these widgets into Layouts:\n",
    "\n",
    "**This will not work properly in Google Colab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_plot = widgets.Output()\n",
    "def on_dropdown(change):\n",
    "    output_plot.clear_output()\n",
    "    with output_plot:\n",
    "        fig, _ = plot_top_results(outputs_prob[change['new']], dls.vocab, show_max=10, figsize=(5,3))\n",
    "        fig.canvas.toolbar_visible = False\n",
    "        fig.tight_layout()\n",
    "dropdown.observe(on_dropdown, names=['index'])\n",
    "\n",
    "widgets.VBox([dropdown, output_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Example\n",
    "\n",
    "Using our trained model and `ipywidgets`, we can even build a small application right here from inside the notebook (inspired an example from the [fast.ai course](https://course.fast.ai/videos/?lesson=3https://course.fast.ai/videos/?lesson=3)).\n",
    "\n",
    "Note: selecting an audio file longer than 5 s and classifying it multiple times might yield different results due to a random cropping in the `ResizeSignal` transform.Note: selecting an audio file longer than 5 s and classifying it multiple times might yield different results due to a random cropping in the `ResizeSignal` transform.\n",
    "\n",
    "**This will not work properly in Google Colab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_DIR/'esc-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload = widgets.FileUpload(accept='.wav')\n",
    "btn_classify = widgets.Button(description='Classify', layout=widgets.Layout(width='300px'))\n",
    "audio_player = widgets.Audio(autoplay=False, loop=False, layout=widgets.Layout(width='300px'))\n",
    "label_upload = widgets.Label(value='No file selected')\n",
    "label_class = widgets.Label(layout=widgets.Layout(width='300px'))\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_click_upload(_):\n",
    "    audio_player.value = btn_upload.data[-1]\n",
    "    label_upload.value = btn_upload.metadata[-1]['name']\n",
    "    btn_classify.description = 'Classify'\n",
    "    \n",
    "def classify(_):\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False)\n",
    "    tmp.file.write(btn_upload.data[-1]); tmp.close()\n",
    "\n",
    "    # load tensor from tmp file and apply transforms\n",
    "    spec = torch.tensor(transforms(tmp.name))\n",
    "    probs = model(spec.unsqueeze(0)).detach().softmax(dim=1).squeeze()\n",
    "    result_idx = probs.argmax()\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        fig, _ = plot_top_results(probs, dls.vocab, show_max=10, figsize=(5, 3))\n",
    "        fig.canvas.header_visible = False\n",
    "        fig.canvas.toolbar_visible = False\n",
    "        fig.tight_layout()\n",
    "\n",
    "    tmp.close(); os.unlink(tmp.name)\n",
    "\n",
    "def reset_button(_):\n",
    "    btn_upload.value.clear()\n",
    "    btn_upload._counter = 1\n",
    "\n",
    "btn_upload.observe(on_click_upload, names=['data'])\n",
    "btn_classify.on_click(classify)\n",
    "btn_upload.observe(reset_button, names=['value'])\n",
    "\n",
    "widgets.VBox([widgets.HBox([btn_upload, label_upload]), audio_player, btn_classify, output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "* [Using the PyTorch C++ Frontent](https://pytorch.org/tutorials/advanced/cpp_frontend.html)\n",
    "* [ML Cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/index.html): brief visual explanations of machine learning concepts with diagrams\n",
    "* [distill.pub](https://distill.pub/): excellent articles about various ML-related topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
